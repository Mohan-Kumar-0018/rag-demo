1. create directory source_documents and add text files
2. create directory db
3. create file load_docs.py and requirements.txt


Prompt :

1. Add Python code in load_docs.py for loading the documents from the file directory.
2. Use DirectoryLoader and TextLoader from langchain_community.document_loaders (langchain_community==0.0.36).
3. For embeddings, use HuggingFaceEmbeddings from langchain_huggingface==0.0.3.
4. Use RecursiveCharacterTextSplitter from langchain.text_splitter (langchain==0.1.17).
5. Use Chroma from langchain_chroma==0.1.4 as the vector store.
6. Store the loaded documents from the source_documents folder.
7. Store the generated embeddings in the db folder.
8. Add the following packages in requirements.txt to ensure compatibility:
torch==2.2.2
sentence-transformers==2.6.1
transformers==4.41.2
huggingface-hub==0.23.0
langchain==0.1.17
langchain_community==0.0.36
langchain_huggingface==0.0.3
langchain_chroma==0.1.4
chromadb==0.4.24
protobuf==4.24.4
pydantic==2.6.4
python-dotenv==1.0.1
tqdm==4.66.1


Create python virtual ENV:
python -m venv venv

Source:
source venv/bin/activate

pip install -r requirements.txt

python load_docs.py

add relevant gitignore files

Create file query_docs.py

Prompt :
1. Add Python code in query_docs.py for query processing using RAG.
2. Convert input query into same embeddings used in load_docs.py using HuggingFaceEmbeddings.
3. Retrieve relevant documents from chroma DB using Chroma.
4. Ollama is installed and running. Use local model llama3.2:latest. Make requests to Ollama API.
5. Add relevant packages to requirements.txt


Load using Docling:
1. Add python code in load_using_docling.py for loading the documents from the file directory.
2. Use Docling to load the documents from the file directory. Have support for pdf and text files. - from docling.document_converter import DocumentConverter, PdfFormatOption
3. For embeddings, use HuggingFaceEmbeddings from langchain_huggingface==0.0.3.
4. Use RecursiveCharacterTextSplitter from langchain.text_splitter (langchain==0.1.17).
5. Use Chroma from langchain_chroma==0.1.4 as the vector store.
6. Store the loaded documents from the source_documents folder.
7. Store the generated embeddings in the db2 folder.
8. Add the docling in requirements.txt


refer load_docs.py and write code in query_docling_docs.py. 
1. Add Python code in query_docling_docs.py for query processing using RAG.
2. Convert input query into same embeddings used in load_docs.py using HuggingFaceEmbeddings.
3. Retrieve relevant documents from chroma DB using Chroma. use db2.
4. Ollama is installed and running. Use local model llama3.2:latest. Make requests to Ollama API.
5. Add relevant packages to requirements.txt


Commands:
1. python load_docs.py
    --verbose
2. python3 query_docs.py --query "your question here"
    --verbose --model llama2 --top_k 5 


Sample queries:

How to submit material info changes on medical conditions?